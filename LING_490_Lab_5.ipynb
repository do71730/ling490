{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/do71730/ling490/blob/main/LING_490_Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_PVqFvjMzz7"
      },
      "source": [
        "Welcome to Lab #5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj5VBVa6Mzz8"
      },
      "source": [
        "Goals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vjY2jZyMzz9"
      },
      "outputs": [],
      "source": [
        "    # 1. Learn how to tag a text for parts of speech (POS) using NLTK\n",
        "    # 2. Learn how to search for trigram collocations in a text using NLTK\n",
        "    # 3. Learn how to search for collocations to specific words in a text using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeW8KxD8Mzz-"
      },
      "source": [
        "Content for Lab #5 is drawn from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_4YUr9qMzz-"
      },
      "outputs": [],
      "source": [
        "    # NLTK Book - Chapter 5: https://www.nltk.org/book/ch05.html\n",
        "    # NLTK Collocations Documentation: http://www.nltk.org/howto/collocations.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuyPYfagMzz-"
      },
      "source": [
        "EXERCISES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "takP0x_iMzz-"
      },
      "outputs": [],
      "source": [
        "    # There are 3 EXERCISES to complete for this lab\n",
        "    # They are found in context throughout the lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opuQ-SLJMzz_"
      },
      "source": [
        "Let's import NLTK and load 'Don Quixote' from Project Gutenberg<br>\n",
        "We will pre-process it so we can then tag it into POS and look for trigram collocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBMP-ghkMzz_",
        "outputId": "97c7312a-f60d-4e04-bc0a-43cb591bb23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import nltk, re, pprint\n",
        "nltk.download('all')\n",
        "from nltk import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_OxJ6ztMzz_",
        "outputId": "dbd0788d-eb64-4aa4-f1b3-820dca8c0a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "1200132\n",
            "﻿The Project Gutenberg eBook of The History of Don Quixote, Vol. I, Complete, by Miguel de Cervantes\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"https://www.gutenberg.org/files/5921/5921-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "print(type(raw))\n",
        "print(len(raw))\n",
        "print(raw[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_72X5m0Mzz_"
      },
      "source": [
        "Tokenize raw text into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEh5jalvMz0A",
        "outputId": "811d7282-217d-4b3a-90b8-c4e1b8a0a340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "250780\n",
            "13952\n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'History', 'of', 'Don', 'Quixote']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print(type(tokens))\n",
        "print(len(tokens))\n",
        "print(len(set(tokens)))\n",
        "print(tokens[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H7J8XZ7Mz0A"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "02S4LBqDMz0A"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    text_nopunct = [c for c in text if c not in string.punctuation]\n",
        "    return text_nopunct\n",
        "# Call the function on a specific text\n",
        "tokens = remove_punctuation(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W47ExEPMz0A"
      },
      "source": [
        "Create an NLTK text from the list of words<br>\n",
        "This allows us to perform all the functions we saw in Chapter 1 and Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yZwdZQwMz0A",
        "outputId": "f138f17d-9495-4120-fe5f-913ea0f98f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.text.Text'>\n",
            "['HAD', 'WITH', 'CERTAIN', 'SKINS', 'OF', 'RED', 'WINE', 'AND', 'BRINGS', 'THE', 'NOVEL', 'OF', '“', 'THE', 'ILL-ADVISED', 'CURIOSITY', '”', 'TO', 'A', 'CLOSE', 'CHAPTER', 'XXXVI', 'WHICH', 'TREATS', 'OF', 'MORE', 'CURIOUS', 'INCIDENTS', 'THAT', 'OCCURRED', 'AT', 'THE', 'INN', 'CHAPTER', 'XXXVII', 'IN', 'WHICH', 'IS']\n",
            "Sancho Panza; Project Gutenberg-tm; thou art; thou wilt; said Sancho;\n",
            "thou hast; Rueful Countenance; del Toboso; Dulcinea del; Project\n",
            "Gutenberg; lady Dulcinea; Holy Brotherhood; United States; thou dost;\n",
            "answered Sancho; Literary Archive; thou wouldst; Gutenberg-tm\n",
            "electronic; galley slaves; Lela Marien\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "text = nltk.Text(tokens)\n",
        "print(type(text))\n",
        "print(text[1024:1062])\n",
        "print(text.collocations())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5vgkTqfMz0B"
      },
      "source": [
        "POS tagging in NLTK<br>\n",
        "Details in Chapter 5 of the NLTK Book<br>\n",
        "NLTK comes with a few POS taggers in different languages<br>\n",
        "and several corpora in multiple languages that are already tagged for POS<br>\n",
        "Once we have a raw text tokenized into words, we can tag these words for POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HLL6PegMz0B",
        "outputId": "21f85ebb-0ba5-484a-f999-e5f456f97b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'History', 'of', 'Don', 'Quixote', 'Vol', 'I', 'Complete', 'by', 'Miguel', 'de', 'Cervantes', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it']\n"
          ]
        }
      ],
      "source": [
        "text_tagged = nltk.pos_tag(text)\n",
        "print(text[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1yS1I-HMz0B"
      },
      "source": [
        "EXERCISE #1<br>\n",
        "Tag a text of your choice for POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-9GeR6rMz0B",
        "outputId": "2209a07e-1470-4e4c-9589-a60120ccd3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'End', 'of', 'Elfintown', 'by', 'Jane', 'Barlow', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're-use']\n"
          ]
        }
      ],
      "source": [
        "url2 = \"https://www.gutenberg.org/cache/epub/67883/pg67883.txt\"\n",
        "response2 = request.urlopen(url2)\n",
        "raw2 = response2.read().decode('utf8')\n",
        "tokens2 = word_tokenize(raw2)\n",
        "tokens2 = remove_punctuation(tokens2)\n",
        "text2 = nltk.Text(tokens2)\n",
        "text_tagged = nltk.pos_tag(text2)\n",
        "print(text2[:50])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwS_W9JMz0B"
      },
      "source": [
        "Collocations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j9nbVSuMz0B"
      },
      "source": [
        "We have looked for bigram collocations<br>\n",
        "but NLTK also has a tool to find trigram collocations<br>\n",
        "Below you find code for bigram and trigram collocations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t01hL_W6Mz0C"
      },
      "source": [
        "Step 1<br>\n",
        "Import the collocation functions from nltk. The * means that we want everything in the 'collocations' functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jQsSKplzMz0C"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ1_u9nsMz0C"
      },
      "source": [
        "Import tools from NLTK to find bigrams and association measures between words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wH1tkPZNMz0C"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.metrics import TrigramAssocMeasures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-uizhnGMz0C"
      },
      "source": [
        "Step 2<br>\n",
        "Find collocations in text<br>\n",
        "Find bigrams, which are pairs of words, and ask for 20 collocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNSgGNV-Mz0C",
        "outputId": "3c16a5e2-0098-4f18-e24b-a0a1baeb8025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Don', 'Quixote'), ('’', 's'), ('”', 'said'), ('”', '“'), ('your', 'worship'), ('of', 'the'), ('I', 'am'), ('he', 'had'), ('it', 'is'), ('I', 'have'), ('Quixote', '“'), ('Don', 'Fernando'), ('have', 'been'), ('in', 'the'), ('to', 'be'), ('DON', 'QUIXOTE'), ('he', 'was'), ('said', 'Don'), ('did', 'not'), ('at', 'once'), ('the', 'same'), ('the', 'curate'), ('Sancho', 'Panza'), ('had', 'been'), ('OF', 'THE'), ('would', 'have'), ('Project', 'Gutenberg-tm'), ('it', 'was'), ('Sancho', '“'), ('”', 'replied'), ('more', 'than'), ('La', 'Mancha'), ('may', 'be'), ('his', 'master'), ('of', 'his'), ('thou', 'art'), ('thou', 'wilt'), ('said', 'Sancho'), ('that', 'he'), ('they', 'were'), ('on', 'the'), ('the', 'world'), ('will', 'be'), ('such', 'a'), ('the', 'of'), ('there', 'is'), ('can', 'not'), ('”', 'answered'), ('has', 'been'), ('’', 't')]\n"
          ]
        }
      ],
      "source": [
        "bigram_finder = BigramCollocationFinder.from_words(text)\n",
        "bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 50)\n",
        "print(bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QJ6TOeRMz0D"
      },
      "source": [
        "Now for Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv9VUG2XMz0D",
        "outputId": "a1a8fb85-97b7-415c-e37b-37773976b785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Don', 'Quixote', '“'), ('said', 'Don', 'Quixote'), ('replied', 'Don', 'Quixote'), ('Don', 'Quixote', '’'), ('“', 'Don', 'Quixote'), ('Señor', 'Don', 'Quixote'), ('answered', 'Don', 'Quixote'), ('Don', 'Quixote', '”'), ('returned', 'Don', 'Quixote'), ('of', 'Don', 'Quixote'), ('which', 'Don', 'Quixote'), ('Don', 'Quixote', 'the'), ('Don', 'Quixote', 'had'), ('Don', 'Quixote', 'perceived'), ('Don', 'Quixote', 'was'), ('Don', 'Quixote', 'and'), ('Don', 'Quixote', 'asked'), ('Don', 'Quixote', 'who'), ('Don', 'Quixote', 'said'), ('Don', 'Quixote', 'bade'), ('”', 'Don', 'Quixote'), ('asked', 'Don', 'Quixote'), ('he', 'Don', 'Quixote'), ('that', 'Don', 'Quixote'), ('Don', 'Quixote', 'replied'), ('lord', 'Don', 'Quixote'), ('Don', 'Quixote', 'Vol'), ('Gentleman', 'Don', 'Quixote'), ('by', 'Don', 'Quixote'), ('Don', 'Quixote', 'gave'), ('Don', 'Quixote', 'a'), ('famous', 'Don', 'Quixote'), ('whereat', 'Don', 'Quixote'), ('a', 'Don', 'Quixote'), ('When', 'Don', 'Quixote'), ('Don', 'Quixote', 'came'), ('Don', 'Quixote', 'hesitated'), ('Don', 'Quixote', 'saw'), ('in', 'Don', 'Quixote'), ('Don', 'Quixote', 'delivered'), ('understood', 'Don', 'Quixote'), ('Don', 'Quixote', 'listened'), ('Don', 'Quixote', 'uttered'), ('where', 'Don', 'Quixote'), ('Don', 'Quixote', 'turned'), ('To', 'Don', 'Quixote'), ('While', 'Don', 'Quixote'), ('knight', 'Don', 'Quixote'), ('smote', 'Don', 'Quixote'), ('Don', 'Quixote', 'renewed')]\n"
          ]
        }
      ],
      "source": [
        "trigram_finder = TrigramCollocationFinder.from_words(text)\n",
        "trigram_finder.nbest(TrigramAssocMeasures.likelihood_ratio, 50)\n",
        "print(trigram_finder.nbest(TrigramAssocMeasures.likelihood_ratio, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JW6wc9TMz0D"
      },
      "source": [
        "Step 3<br>\n",
        "Let's eliminate the stopwords<br>\n",
        "The output looks much better and the collocations seem much more likely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E51uVFDEMz0D"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTDcAZzGMz0D",
        "outputId": "cd57eebc-a4d4-48a7-9d09-f002eda015f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Don', 'Quixote'),\n",
              " ('Don', 'Fernando'),\n",
              " ('DON', 'QUIXOTE'),\n",
              " ('said', 'Don'),\n",
              " ('Sancho', 'Panza'),\n",
              " ('Project', 'Gutenberg-tm'),\n",
              " ('thou', 'art'),\n",
              " ('thou', 'wilt'),\n",
              " ('said', 'Sancho'),\n",
              " ('thou', 'hast'),\n",
              " ('Rueful', 'Countenance'),\n",
              " ('del', 'Toboso'),\n",
              " ('Dulcinea', 'del'),\n",
              " ('Project', 'Gutenberg'),\n",
              " ('WITH', 'OTHER'),\n",
              " ('WHICH', 'TREATS'),\n",
              " ('lady', 'Dulcinea'),\n",
              " ('Holy', 'Brotherhood'),\n",
              " ('Don', 'Luis'),\n",
              " ('United', 'States'),\n",
              " ('replied', 'Don'),\n",
              " ('thou', 'dost'),\n",
              " ('TOGETHER', 'WITH'),\n",
              " ('answered', 'Sancho'),\n",
              " ('Literary', 'Archive'),\n",
              " ('thou', 'wouldst'),\n",
              " ('Gutenberg-tm', 'electronic'),\n",
              " ('galley', 'slaves'),\n",
              " ('Lela', 'Marien'),\n",
              " ('thou', 'canst'),\n",
              " ('Archive', 'Foundation'),\n",
              " ('electronic', 'works'),\n",
              " ('Gutenberg', 'Literary'),\n",
              " ('good', 'fortune'),\n",
              " ('thou', 'mayest'),\n",
              " ('thou', 'shalt'),\n",
              " ('SANCHO', 'PANZA'),\n",
              " ('First', 'Part'),\n",
              " ('Second', 'Part'),\n",
              " ('THE', 'INN'),\n",
              " ('next', 'day'),\n",
              " ('Dona', 'Clara'),\n",
              " ('thou', 'knowest'),\n",
              " ('young', 'man'),\n",
              " ('Master', 'Nicholas'),\n",
              " ('Señor', 'Don'),\n",
              " ('even', 'though'),\n",
              " ('answered', 'Don'),\n",
              " ('SIERRA', 'MORENA'),\n",
              " ('Princess', 'Micomicona')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "stopset = set(stopwords.words('english'))\n",
        "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
        "bigram_finder.apply_word_filter(filter_stops)\n",
        "bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-mrPHhWMz0D",
        "outputId": "bb1ed0a4-9b1c-4e9b-bacf-b901ced56fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Don', 'Quixote'), ('Don', 'Fernando'), ('DON', 'QUIXOTE'), ('said', 'Don'), ('Sancho', 'Panza'), ('Project', 'Gutenberg-tm'), ('thou', 'art'), ('thou', 'wilt'), ('said', 'Sancho'), ('thou', 'hast'), ('Rueful', 'Countenance'), ('del', 'Toboso'), ('Dulcinea', 'del'), ('Project', 'Gutenberg'), ('WITH', 'OTHER'), ('WHICH', 'TREATS'), ('lady', 'Dulcinea'), ('Holy', 'Brotherhood'), ('Don', 'Luis'), ('United', 'States'), ('replied', 'Don'), ('thou', 'dost'), ('TOGETHER', 'WITH'), ('answered', 'Sancho'), ('Literary', 'Archive'), ('thou', 'wouldst'), ('Gutenberg-tm', 'electronic'), ('galley', 'slaves'), ('Lela', 'Marien'), ('thou', 'canst'), ('Archive', 'Foundation'), ('electronic', 'works'), ('Gutenberg', 'Literary'), ('good', 'fortune'), ('thou', 'mayest'), ('thou', 'shalt'), ('SANCHO', 'PANZA'), ('First', 'Part'), ('Second', 'Part'), ('THE', 'INN'), ('next', 'day'), ('Dona', 'Clara'), ('thou', 'knowest'), ('young', 'man'), ('Master', 'Nicholas'), ('Señor', 'Don'), ('even', 'though'), ('answered', 'Don'), ('SIERRA', 'MORENA'), ('Princess', 'Micomicona')]\n"
          ]
        }
      ],
      "source": [
        "print(bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4h_CivIMz0E",
        "outputId": "22476759-6515-4e96-831b-3f80a04017c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said', 'Don', 'Quixote'),\n",
              " ('replied', 'Don', 'Quixote'),\n",
              " ('Señor', 'Don', 'Quixote'),\n",
              " ('answered', 'Don', 'Quixote'),\n",
              " ('returned', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'perceived'),\n",
              " ('Don', 'Quixote', 'asked'),\n",
              " ('Don', 'Quixote', 'said'),\n",
              " ('Don', 'Quixote', 'bade'),\n",
              " ('asked', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'replied'),\n",
              " ('lord', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'Vol'),\n",
              " ('Gentleman', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'gave'),\n",
              " ('famous', 'Don', 'Quixote'),\n",
              " ('whereat', 'Don', 'Quixote'),\n",
              " ('When', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'came'),\n",
              " ('Don', 'Quixote', 'hesitated'),\n",
              " ('Don', 'Quixote', 'saw'),\n",
              " ('Don', 'Quixote', 'delivered'),\n",
              " ('understood', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'listened'),\n",
              " ('Don', 'Quixote', 'uttered'),\n",
              " ('Don', 'Quixote', 'turned'),\n",
              " ('While', 'Don', 'Quixote'),\n",
              " ('knight', 'Don', 'Quixote'),\n",
              " ('smote', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'renewed'),\n",
              " ('Don', 'Quixote', 'stood'),\n",
              " ('discourse', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'guessing'),\n",
              " ('Don', 'Quixote', 'merrily'),\n",
              " ('Piedrahita', 'Don', 'Quixote'),\n",
              " ('alms', 'Don', 'Quixote'),\n",
              " ('dolefully', 'Don', 'Quixote'),\n",
              " ('invests', 'Don', 'Quixote'),\n",
              " ('oppressively', 'Don', 'Quixote'),\n",
              " ('twenty-one', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'released'),\n",
              " ('Don', 'Quixote', 'still'),\n",
              " ('valiant', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'observing'),\n",
              " ('But', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'took'),\n",
              " ('conversation', 'Don', 'Quixote'),\n",
              " ('BELTENEBROS', 'Don', 'Quixote'),\n",
              " ('Don', 'Quixote', 'descried'),\n",
              " ('Don', 'Quixote', 'fuming')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "stopset = set(stopwords.words('english'))\n",
        "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
        "trigram_finder.apply_word_filter(filter_stops)\n",
        "trigram_finder.nbest(TrigramAssocMeasures.likelihood_ratio, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0K6WyU7Mz0E",
        "outputId": "4680b417-7d78-4675-abe8-118d33e0af64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('said', 'Don', 'Quixote'), ('replied', 'Don', 'Quixote'), ('Señor', 'Don', 'Quixote'), ('answered', 'Don', 'Quixote'), ('returned', 'Don', 'Quixote'), ('Don', 'Quixote', 'perceived'), ('Don', 'Quixote', 'asked'), ('Don', 'Quixote', 'said'), ('Don', 'Quixote', 'bade'), ('asked', 'Don', 'Quixote'), ('Don', 'Quixote', 'replied'), ('lord', 'Don', 'Quixote'), ('Don', 'Quixote', 'Vol'), ('Gentleman', 'Don', 'Quixote'), ('Don', 'Quixote', 'gave'), ('famous', 'Don', 'Quixote'), ('whereat', 'Don', 'Quixote'), ('When', 'Don', 'Quixote'), ('Don', 'Quixote', 'came'), ('Don', 'Quixote', 'hesitated'), ('Don', 'Quixote', 'saw'), ('Don', 'Quixote', 'delivered'), ('understood', 'Don', 'Quixote'), ('Don', 'Quixote', 'listened'), ('Don', 'Quixote', 'uttered'), ('Don', 'Quixote', 'turned'), ('While', 'Don', 'Quixote'), ('knight', 'Don', 'Quixote'), ('smote', 'Don', 'Quixote'), ('Don', 'Quixote', 'renewed'), ('Don', 'Quixote', 'stood'), ('discourse', 'Don', 'Quixote'), ('Don', 'Quixote', 'guessing'), ('Don', 'Quixote', 'merrily'), ('Piedrahita', 'Don', 'Quixote'), ('alms', 'Don', 'Quixote'), ('dolefully', 'Don', 'Quixote'), ('invests', 'Don', 'Quixote'), ('oppressively', 'Don', 'Quixote'), ('twenty-one', 'Don', 'Quixote'), ('Don', 'Quixote', 'released'), ('Don', 'Quixote', 'still'), ('valiant', 'Don', 'Quixote'), ('Don', 'Quixote', 'observing'), ('But', 'Don', 'Quixote'), ('Don', 'Quixote', 'took'), ('conversation', 'Don', 'Quixote'), ('BELTENEBROS', 'Don', 'Quixote'), ('Don', 'Quixote', 'descried'), ('Don', 'Quixote', 'fuming')]\n"
          ]
        }
      ],
      "source": [
        "print(trigram_finder.nbest(TrigramAssocMeasures.likelihood_ratio, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mny28dYHMz0E"
      },
      "source": [
        "EXERCISE #2<br>\n",
        "Find bigram and trigram collocations to a text of your choice<br>\n",
        "Report the most likely 50 collocations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_finder2 = BigramCollocationFinder.from_words(text2)\n",
        "stopset2 = set(stopwords.words('english'))\n",
        "filter_stops2 = lambda w: len(w) < 3 or w in stopset2\n",
        "bigram_finder2.apply_word_filter(filter_stops2)\n",
        "bigram_finder2.nbest(BigramAssocMeasures.likelihood_ratio, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOGOHLVvRrUT",
        "outputId": "cf38e65f-3815-43d9-fbb2-4e676a10bd12"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg'),\n",
              " ('United', 'States'),\n",
              " ('Literary', 'Archive'),\n",
              " ('Gutenberg', 'Literary'),\n",
              " ('Archive', 'Foundation'),\n",
              " ('Gutenberg-tm', 'electronic'),\n",
              " ('electronic', 'works'),\n",
              " ('set', 'forth'),\n",
              " ('electronic', 'work'),\n",
              " ('Macmillan', 'Co._'),\n",
              " ('Illustration', '_Copyright'),\n",
              " ('_Copyright', '1894'),\n",
              " ('Gutenberg-tm', 'License'),\n",
              " ('_Fays', 'following._'),\n",
              " ('_Fays', 'leading._'),\n",
              " ('copyright', 'law'),\n",
              " ('Bad', 'Brown'),\n",
              " ('copyright', 'holder'),\n",
              " ('PROJECT', 'GUTENBERG'),\n",
              " ('Brown', 'Witch'),\n",
              " ('You', 'may'),\n",
              " ('U.S.', 'copyright'),\n",
              " ('AGREE', 'THAT'),\n",
              " ('Jane', 'Barlow'),\n",
              " ('Plain', 'Vanilla'),\n",
              " ('Vanilla', 'ASCII'),\n",
              " ('considerable', 'effort'),\n",
              " ('intellectual', 'property'),\n",
              " ('restrictions', 'whatsoever'),\n",
              " ('visit', 'www.gutenberg.org/donate'),\n",
              " ('free', 'distribution'),\n",
              " ('THE', 'END'),\n",
              " ('What', 'time'),\n",
              " ('paragraph', '1.F.3'),\n",
              " ('derivative', 'works'),\n",
              " ('GUTENBERG', 'EBOOK'),\n",
              " ('General', 'Terms'),\n",
              " ('Royalty', 'payments'),\n",
              " ('active', 'links'),\n",
              " ('public', 'domain'),\n",
              " ('wondrous', 'thing'),\n",
              " ('written', 'explanation'),\n",
              " ('trademark', 'license'),\n",
              " ('exempt', 'status'),\n",
              " ('legal', 'fees'),\n",
              " ('full', 'Project'),\n",
              " ('anyone', 'anywhere'),\n",
              " ('physical', 'medium'),\n",
              " ('creating', 'derivative')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_finder2 = TrigramCollocationFinder.from_words(text2)\n",
        "trigram_finder2.apply_word_filter(filter_stops2)\n",
        "trigram_finder2.nbest(TrigramAssocMeasures.likelihood_ratio, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO62TN0oRxE7",
        "outputId": "5cbbce22-3430-4cd5-bfef-30586e5529f8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Project', 'Gutenberg-tm', 'electronic'),\n",
              " ('Project', 'Gutenberg-tm', 'License'),\n",
              " ('full', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', 'trademark'),\n",
              " ('Project', 'Gutenberg-tm', 'works'),\n",
              " ('Project', 'Gutenberg-tm', 'work'),\n",
              " ('Project', 'Gutenberg-tm', 'eBooks'),\n",
              " ('individual', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', 'collection'),\n",
              " ('Project', 'Gutenberg-tm', 'mission'),\n",
              " ('distributing', 'Project', 'Gutenberg-tm'),\n",
              " ('About', 'Project', 'Gutenberg-tm'),\n",
              " ('Full', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', '1.E.5'),\n",
              " ('Project', 'Gutenberg-tm', 'depends'),\n",
              " ('Redistributing', 'Project', 'Gutenberg-tm'),\n",
              " ('sharing', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', 'concept'),\n",
              " ('Project', 'Gutenberg-tm', 'name'),\n",
              " ('efforts', 'Project', 'Gutenberg-tm'),\n",
              " ('official', 'Project', 'Gutenberg-tm'),\n",
              " ('distributed', 'Project', 'Gutenberg-tm'),\n",
              " ('support', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', 'website'),\n",
              " ('Project', 'Gutenberg-tm', 'including'),\n",
              " ('Foundation', 'Project', 'Gutenberg-tm'),\n",
              " ('Gutenberg-tm', 'Project', 'Gutenberg-tm'),\n",
              " ('Project', 'Gutenberg-tm', 'Project'),\n",
              " ('Project', 'Gutenberg', 'Literary'),\n",
              " ('Project', 'Gutenberg', 'License'),\n",
              " ('Literary', 'Archive', 'Foundation'),\n",
              " ('Gutenberg', 'Literary', 'Archive'),\n",
              " ('Gutenberg-tm', 'electronic', 'works'),\n",
              " ('Project', 'Gutenberg', 'trademark'),\n",
              " ('Project', 'Gutenberg', 'eBooks'),\n",
              " ('1.F.1', 'Project', 'Gutenberg'),\n",
              " ('Project', 'Gutenberg', 'web'),\n",
              " ('\\ufeffThe', 'Project', 'Gutenberg'),\n",
              " ('research', 'Project', 'Gutenberg'),\n",
              " ('The', 'Project', 'Gutenberg'),\n",
              " ('Project', 'Gutenberg', 'volunteers'),\n",
              " ('Project', 'Gutenberg', '1.E.1'),\n",
              " ('Project', 'Gutenberg', 'eBook'),\n",
              " ('trademark', 'Project', 'Gutenberg'),\n",
              " ('United', 'States', 'without'),\n",
              " ('Gutenberg-tm', 'electronic', 'work'),\n",
              " ('United', 'States', 'Compliance'),\n",
              " ('United', 'States', '1.E'),\n",
              " ('United', 'States', 'check'),\n",
              " ('United', 'States', 'U.S.')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgOerSvZMz0E"
      },
      "source": [
        "We can look for collocations to specific words too<br>\n",
        "Let's search for collocations around the word 'rona' in the few_tweets.csv file<br>\n",
        "We need to load the text in and pre-process it before we can ask for collocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fneb_1idMz0E"
      },
      "outputs": [],
      "source": [
        "My_file = open(\"few_tweets.csv\")\n",
        "raw = My_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "IXxgJAe_Mz0E"
      },
      "outputs": [],
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wquqKuMz0F"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ombHg-F3Mz0F"
      },
      "source": [
        "Make all words lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgkM43htMz0F",
        "outputId": "2209cc46-bf82-4164-e60c-31ccac585144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",text,,\n",
            "1,\"staring: rona... action, drama, and suspense 🎬 https://t.co/jrnphgphx3\",,\n",
            "2,if my driving test gets cancelled because miss rona wants to move mad i’m actually gonna drive anyway. the police will be busy dealing with the revolution anyway so.... https://t.co/dss0p4ujml,\n",
            "9079\n"
          ]
        }
      ],
      "source": [
        "raw = raw.lower()\n",
        "print(raw[:280])\n",
        "print(len(raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN5gn22HMz0F"
      },
      "source": [
        "Tokenize the lowercase text into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE1P4u5WMz0F",
        "outputId": "6830b2ce-33f8-4482-ed4f-689bd785e819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', 'text', ',', ',', '1', ',', \"''\", 'staring', ':', 'rona', '...', 'action', ',', 'drama', ',', 'and', 'suspense', '🎬', 'https', ':', '//t.co/jrnphgphx3', \"''\", ',', ',', '2', ',', 'if', 'my', 'driving', 'test', 'gets', 'cancelled', 'because', 'miss', 'rona', 'wants', 'to', 'move', 'mad', 'i', '’', 'm', 'actually', 'gon', 'na', 'drive', 'anyway', '.', 'the', 'police', 'will', 'be', 'busy', 'dealing', 'with', 'the', 'revolution', 'anyway', 'so', '...', '.', 'https', ':', '//t.co/dss0p4ujml', ',', ',', '3', ',', 'aaron', 'rodgers', 'get', 'that', 'rona', 'i', '’', 'm', 'dropping', '81', 'on', 'his', 'head', 'nigga', ',', ',', '4', ',', 'good', 'old', 'rona', 'sorted', 'me', 'right', 'out', 'here', 'https', ':', '//t.co/xadli1cnhp', ',', ',', '5', ',', 'on', 'top', 'of', 'rona', 'the', 'pollen', 'out', 'here', 'like', 'https', ':', '//t.co/s6jynjr1tg', ',', ',', '6', ',', 'after', 'reading', 'this', 'i', 'think', 'i', 'may', 'have', 'had', 'that', 'rona', 'back', 'in', 'january', '.', 'same', 'symptoms', 'as', 'here', 'but', 'it', 'ended', 'with', 'whatever', 'was', 'infecting', 'me', 'dissolving', 'a', 'benign', 'breast', 'tumor', 'i', '’', 'd', 'had', 'for', 'months', 'that', 'doctors', 'said', 'i', '’', 'd', 'have', 'to', 'get', 'surgically', 'removed', '...', 'medical', 'twitter', 'can', 'u', 'explain', '?', 'https', ':', '//t.co/jvyemqunz3', ',', ',', '7', ',', \"''\", '12,000', 'confirmed', 'rona', 'cases', '.', 'it', 'will', 'reach', 'abt', '30,000', \"''\", ',', ',', '8', ',', '@', 'ramosmelissaaa', 'yeah', 'you', 'getting', 'the', 'rona', ',', ',', '9', ',', 'sierra', 'leone', 'has', '0', 'rona', 'cases', '?', '?', '?', 'https', ':', '//t.co/mn8npnqwuy', ',', ',', '10', ',', 'big', 'rona', 'do', \"n't\", 'stop', 'the', 'tea', 'https', ':', '//t.co/wrv4ey4qtq', ',', ',', '11', ',', 'theses', 'kids', 'look', 'like', 'busted', 'trailer', 'trash', '...', 'big', 'rona', 'can', 'take', 'them', '¯\\\\_', '(', 'ツ', ')', '_/¯', 'https', ':', '//t.co/usjzoyndk3', ',', ',', '12', ',', \"''\", 'white', 'people', ':', 'covid-19', 'black', 'folks', ':', 'the', 'rona', 'gay', 'people', ':', 'miss', 'rona', '😭☠️', \"''\", ',']\n",
            "2186\n"
          ]
        }
      ],
      "source": [
        "Tweets_words = word_tokenize(raw)\n",
        "print(Tweets_words[:280])\n",
        "print(len(Tweets_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTeqFLKTMz0F"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YE0HYlQuMz0F"
      },
      "outputs": [],
      "source": [
        "Tweets_words = remove_punctuation(Tweets_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmEvNTHbMz0F"
      },
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "WdqIAETkMz0G"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "Tweets_words = [w for w in Tweets_words if not w in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-30k13yWMz0G"
      },
      "source": [
        "Now we can search for collocations around the word 'rona'<br>\n",
        "Ngrams with 'rona' as a member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kiG2K2cgMz0G"
      },
      "outputs": [],
      "source": [
        "rona_filter = lambda *w: 'rona' not in w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FRs1Jl7Mz0G"
      },
      "source": [
        " Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6v6jGngMz0G",
        "outputId": "12d12689-4cfd-460b-8469-2989865ba308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('miss', 'rona'), ('big', 'rona'), ('rona', 'pulling'), ('ms', 'rona'), ('‘', 'rona'), ('rona', '’'), ('rona', \"n't\"), ('got', 'rona'), ('rona', '...'), ('get', 'rona'), ('rona', 'got'), ('rona', 'https')]\n"
          ]
        }
      ],
      "source": [
        "finder = BigramCollocationFinder.from_words(\n",
        "   Tweets_words)\n",
        "# only bigrams that appear 3+ times\n",
        "finder.apply_freq_filter(3)\n",
        "# only bigrams that contain 'creature'\n",
        "finder.apply_ngram_filter(rona_filter)\n",
        "# return the 10 n-grams with the highest likelihood\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEzs-VR8Mz0G"
      },
      "source": [
        " Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDAA2oBMMz0G",
        "outputId": "24b245c6-a3d7-4e6f-efb0-874d32c837b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rona', 'pulling', 'spring'), ('papoose', '‘', 'rona'), ('‘', 'rona', '’'), ('rona', '’', 'freestyle'), ('rona', 'got', 'fucked')]\n"
          ]
        }
      ],
      "source": [
        "finder = TrigramCollocationFinder.from_words(\n",
        "   Tweets_words)\n",
        "# only trigrams that appear 3+ times\n",
        "finder.apply_freq_filter(3)\n",
        "# only trigrams that contain 'creature'\n",
        "finder.apply_ngram_filter(rona_filter)\n",
        "# return the 10 n-grams with the highest likelihood\n",
        "print(finder.nbest(trigram_measures.likelihood_ratio, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZaXP9A2Mz0H"
      },
      "source": [
        "EXERCISE #3<br>\n",
        "Search for collocations to 5 specific words of your choice.<br>\n",
        "Report the 20 most likely collocations to these 5 words.<br>\n",
        "You can use the few_tweets.csv file which contains 100 tweets containing<br>\n",
        "the word rona, or you can try this out on a text of your choice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OHE1KOz3ViTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyKVypP4Mz0H"
      },
      "source": [
        "Coming up<br>\n",
        "In the final NLTK Lab, Lab #6, we will learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTfVA1-HMz0H"
      },
      "outputs": [],
      "source": [
        "    # How to search a text for specific parts of speech\n",
        "        # We can look for the most frequent verbs or nouns for instance\n",
        "    # Combining POS tagging with our tool to find collocations\n",
        "        # We can look for collocations by part of speech just like we\n",
        "        # did for words\n",
        "        # This means that we can look for bigram collocations for 'Adjective-Noun'\n",
        "        # combinations (e.g. powerful symbol, powerful lobby)\n",
        "    # How to access Twitter with Python so you can do your own searches and download your own data\n",
        "    # How to perform a (live) sentiment analysis on your Twitter search results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LING_490_Lab_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}