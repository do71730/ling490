{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/do71730/ling490/blob/main/LING_490_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daWOcbWbW-B2"
      },
      "source": [
        "Welcome to Lab #7 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVWmbFQIW-B4"
      },
      "source": [
        "Goals #<br>\n",
        "1. Search a text for bigram collocations by part of speech (POS)<br>\n",
        "  a. See EXERCISE 1<br>\n",
        "2. Get a conditional frequency distribution for each type of part of speech (POS)<br>\n",
        "  a. See EXERCISE 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5zVuk99W-B4"
      },
      "source": [
        "We need a text to analyze...<br>\n",
        "Let's import NLTK and load 'Don Quixote' from Project Gutenberg<br>\n",
        "We will pre-process it so we can then tag it into POS and look for collocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFvxUahcW-B5",
        "outputId": "c84fde6b-3021-4586-b4a7-11b98a624b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk, re, pprint\n",
        "nltk.download(\"all\")\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W4_-L1pW-B6"
      },
      "source": [
        "Get a text from project gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTnNNCiNW-B6"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "url = \"https://www.gutenberg.org/files/5921/5921-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "print(type(raw))\n",
        "print(len(raw))\n",
        "print(raw[:75])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABRGX-TW-B6"
      },
      "source": [
        "Tokenize raw text into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pyJmgTDW-B7"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print(type(tokens))\n",
        "print(len(tokens))\n",
        "print(len(set(tokens)))\n",
        "print(tokens[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp10uzUZW-B7"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0qtpzo8W-B8"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    text_nopunct = [c for c in text if c not in string.punctuation]\n",
        "    return text_nopunct\n",
        "# Call the function on a specific text\n",
        "tokens = remove_punctuation(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzb7QzBxW-B8"
      },
      "source": [
        "Create an NLTK text from the list of words<br>\n",
        "This allows us to perform all the functions we saw in Chapter 1 and Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKoapkzgW-B9"
      },
      "outputs": [],
      "source": [
        "text = nltk.Text(tokens)\n",
        "print(type(text))\n",
        "print(text[1024:1062])\n",
        "print(text.collocations())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqzR_87YW-B9"
      },
      "source": [
        "POS tagging in NLTK<br>\n",
        "Once we have a raw text tokenized into words, we can tag these words for POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k_8ei8fW-B9"
      },
      "outputs": [],
      "source": [
        "text_tagged = nltk.pos_tag(text)\n",
        "print(text_tagged[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV16sPz3W-B-"
      },
      "source": [
        "# SEARCH FOR COLLOCATIONS BY POS ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8n3TcLaW-B-"
      },
      "source": [
        "Find the most likely collocations by POS<br>\n",
        "This returns only the POS tags without words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVanxcQrW-B-"
      },
      "outputs": [],
      "source": [
        "finder = BigramCollocationFinder.from_words(t for w, t in\n",
        "    text_tagged)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdm4h9wW-B-"
      },
      "source": [
        "Search for collocations by POS #<br>\n",
        "Use this word filter to select the type of bigram collocation you want #<br>\n",
        "Consult the Google Doc 'Lab #7' for a list of POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXBzfjh1W-B-"
      },
      "outputs": [],
      "source": [
        "word_filter = lambda w1, w2: not ((w1[1] == \"VB\") and (w2[1] == \"NN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPTUjMmBW-B_"
      },
      "source": [
        "Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VB4ip7tW-B_"
      },
      "outputs": [],
      "source": [
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged)\n",
        "# only bigrams that appear 3+ times\n",
        "finder.apply_freq_filter(3)\n",
        "# only bigrams that contain the type of bigram collocation that you want\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "# return the 30 n-grams with the highest likelihood\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mar2HgWmW-B_"
      },
      "source": [
        " EXERCISE 1 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAtaIs6hW-B_"
      },
      "source": [
        "1. Select a text to analyze and pre-process it using NLTK<br>\n",
        "2. Search for collocations based on POS<br>\n",
        "  a. Examine the most likely collocation types by POS<br>\n",
        "  b. Search for bigram collocations of the type 'Adjective-Noun' like 'powerful lobby'<br>\n",
        "  c. Search for bigram collocations of the type 'Verb-Noun' like 'eat pizza'<br>\n",
        "  d. Search for bigram collocations of the type of your choice<br>\n",
        "  e. Search for bigram collocations of the type of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcf6lTMSW-CA"
      },
      "source": [
        "# GET THE FREQUENCY OF EACH POS IN A TEXT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYdMq05W-CA"
      },
      "source": [
        "The following code was taken for our NLTK book, Chapter 5, Section 2.7<br>\n",
        "'Unsimplified Tags': https://www.nltk.org/book/ch05.html<br>\n",
        "Consult that section and that chapter to learn more about working with tagged corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_5WKJNlW-CA"
      },
      "source": [
        "Find the most frequent nouns of each noun POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C_95u_IW-CA"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq1Uk1pIW-CA"
      },
      "outputs": [],
      "source": [
        "tagdict = findtags('NN', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDT6f1k8W-CA"
      },
      "source": [
        "Find the most frequent verbs of each verb POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvFEIk3iW-CB"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He_EV47SW-CB"
      },
      "outputs": [],
      "source": [
        "tagdict = findtags('VB', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELTXWBfW-CB"
      },
      "source": [
        "Find the most frequent adjectives of each adjective POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6egXJpnW-CB"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ucSmh3IW-CB"
      },
      "outputs": [],
      "source": [
        "tagdict = findtags('JJ', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkla2WJIW-CB"
      },
      "source": [
        "Find the most frequent adverbs of each adverb POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9knfLKlW-CC"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2prhliF_W-CC"
      },
      "outputs": [],
      "source": [
        "tagdict = findtags('RB', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4PTuHbW-CC"
      },
      "source": [
        " EXERCISE 2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaHclYdjW-CC"
      },
      "source": [
        "1. Select a text to analyze and pre-process it using NLTK. It can be the same text<br>\n",
        "      that you used for EXERCISE 1<br>\n",
        "2. Find the 10 most frequent types of:<br>\n",
        "  a. nouns<br>\n",
        "  b. verbs<br>\n",
        "  c. adjectives<br>\n",
        "  d. adverbs<br>\n",
        "  e. a POS category of your choice<br>\n",
        "  f. a POS category of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWsxPX5CW-CC"
      },
      "source": [
        " END LAB"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LING_490_Lab_7.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}