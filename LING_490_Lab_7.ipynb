{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/do71730/ling490/blob/main/LING_490_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daWOcbWbW-B2"
      },
      "source": [
        "Welcome to Lab #7 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVWmbFQIW-B4"
      },
      "source": [
        "Goals #<br>\n",
        "1. Search a text for bigram collocations by part of speech (POS)<br>\n",
        "  a. See EXERCISE 1<br>\n",
        "2. Get a conditional frequency distribution for each type of part of speech (POS)<br>\n",
        "  a. See EXERCISE 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5zVuk99W-B4"
      },
      "source": [
        "We need a text to analyze...<br>\n",
        "Let's import NLTK and load 'Don Quixote' from Project Gutenberg<br>\n",
        "We will pre-process it so we can then tag it into POS and look for collocations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFvxUahcW-B5",
        "outputId": "33756ea2-15eb-4259-9a74-35e12a9bd043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import nltk, re, pprint\n",
        "nltk.download(\"all\")\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W4_-L1pW-B6"
      },
      "source": [
        "Get a text from project gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "RTnNNCiNW-B6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b232e6b3-20c1-4f1a-d726-ca21e51125fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "1200132\n",
            "﻿The Project Gutenberg eBook of The History of Don Quixote, Vol. I, Complet\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"https://www.gutenberg.org/files/5921/5921-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "print(type(raw))\n",
        "print(len(raw))\n",
        "print(raw[:75])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABRGX-TW-B6"
      },
      "source": [
        "Tokenize raw text into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_pyJmgTDW-B7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d016da-ac7f-4e13-89c8-f3e9b2214870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "250780\n",
            "13952\n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'History', 'of', 'Don', 'Quixote']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print(type(tokens))\n",
        "print(len(tokens))\n",
        "print(len(set(tokens)))\n",
        "print(tokens[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp10uzUZW-B7"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "t0qtpzo8W-B8"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    text_nopunct = [c for c in text if c not in string.punctuation]\n",
        "    return text_nopunct\n",
        "# Call the function on a specific text\n",
        "tokens = remove_punctuation(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzb7QzBxW-B8"
      },
      "source": [
        "Create an NLTK text from the list of words<br>\n",
        "This allows us to perform all the functions we saw in Chapter 1 and Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xKoapkzgW-B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40b49fc-fa75-489e-af7e-4eae89acef55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.text.Text'>\n",
            "['HAD', 'WITH', 'CERTAIN', 'SKINS', 'OF', 'RED', 'WINE', 'AND', 'BRINGS', 'THE', 'NOVEL', 'OF', '“', 'THE', 'ILL-ADVISED', 'CURIOSITY', '”', 'TO', 'A', 'CLOSE', 'CHAPTER', 'XXXVI', 'WHICH', 'TREATS', 'OF', 'MORE', 'CURIOUS', 'INCIDENTS', 'THAT', 'OCCURRED', 'AT', 'THE', 'INN', 'CHAPTER', 'XXXVII', 'IN', 'WHICH', 'IS']\n",
            "Sancho Panza; Project Gutenberg-tm; thou art; thou wilt; said Sancho;\n",
            "thou hast; Rueful Countenance; del Toboso; Dulcinea del; Project\n",
            "Gutenberg; lady Dulcinea; Holy Brotherhood; United States; thou dost;\n",
            "answered Sancho; Literary Archive; thou wouldst; Gutenberg-tm\n",
            "electronic; galley slaves; Lela Marien\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "text = nltk.Text(tokens)\n",
        "print(type(text))\n",
        "print(text[1024:1062])\n",
        "print(text.collocations())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqzR_87YW-B9"
      },
      "source": [
        "POS tagging in NLTK<br>\n",
        "Once we have a raw text tokenized into words, we can tag these words for POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-k_8ei8fW-B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1fdeb9-cac0-45e5-9c9c-74f8fe1cb753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\\ufeffThe', 'NN'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('The', 'DT'), ('History', 'NNP'), ('of', 'IN'), ('Don', 'NNP'), ('Quixote', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "text_tagged = nltk.pos_tag(text)\n",
        "print(text_tagged[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV16sPz3W-B-"
      },
      "source": [
        "# SEARCH FOR COLLOCATIONS BY POS ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8n3TcLaW-B-"
      },
      "source": [
        "Find the most likely collocations by POS<br>\n",
        "This returns only the POS tags without words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GVanxcQrW-B-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdee8f7-b2db-4999-9c08-48a186434faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('DT', 'NN'), ('TO', 'VB'), ('NNP', 'NNP'), ('IN', 'DT'), ('MD', 'VB'), ('PRP', 'VBD'), ('PRP$', 'NN'), ('PRP', 'VBP'), ('NN', 'IN'), ('JJ', 'NN'), ('PRP', 'MD'), ('NN', 'CC'), ('DT', 'JJ'), ('PRP', 'NN'), ('IN', 'PRP$'), ('DT', 'DT'), ('DT', 'IN'), ('IN', 'VBD'), ('IN', 'IN'), ('IN', 'VB')]\n"
          ]
        }
      ],
      "source": [
        "finder = BigramCollocationFinder.from_words(t for w, t in\n",
        "    text_tagged)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdm4h9wW-B-"
      },
      "source": [
        "Search for collocations by POS #<br>\n",
        "Use this word filter to select the type of bigram collocation you want #<br>\n",
        "Consult the Google Doc 'Lab #7' for a list of POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vXBzfjh1W-B-"
      },
      "outputs": [],
      "source": [
        "word_filter = lambda w1, w2: not ((w1[1] == \"VB\") and (w2[1] == \"NN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPTUjMmBW-B_"
      },
      "source": [
        "Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-VB4ip7tW-B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f7510f-0884-4ae7-cc25-2452ee869533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('take', 'VB'), ('care', 'NN')), (('give', 'VB'), ('thee', 'NN')), (('make', 'VB'), ('use', 'NN')), (('take', 'VB'), ('vengeance', 'NN')), (('give', 'VB'), ('aid', 'NN')), (('have', 'VB'), ('recourse', 'NN')), (('thou', 'VB'), ('hast', 'NN')), (('wait', 'VB'), ('till', 'NN')), (('say', 'VB'), ('anything', 'NN')), (('draw', 'VB'), ('sword', 'NN')), (('thou', 'VB'), ('wouldst', 'NN')), (('tell', 'VB'), ('thee', 'NN')), (('thou', 'VB'), ('art', 'NN')), (('take', 'VB'), ('leave', 'NN')), (('come', 'VB'), ('forth', 'NN')), (('take', 'VB'), ('place', 'NN')), (('give', 'VB'), ('way', 'NN')), (('say', 'VB'), ('“', 'NN')), (('go', 'VB'), ('home', 'NN')), (('say', 'VB'), ('nothing', 'NN')), (('be', 'VB'), ('anything', 'NN')), (('have', 'VB'), ('time', 'NN')), (('be', 'VB'), ('something', 'NN')), (('be', 'VB'), ('time', 'NN'))]\n"
          ]
        }
      ],
      "source": [
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged)\n",
        "# only bigrams that appear 3+ times\n",
        "finder.apply_freq_filter(3)\n",
        "# only bigrams that contain the type of bigram collocation that you want\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "# return the 30 n-grams with the highest likelihood\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mar2HgWmW-B_"
      },
      "source": [
        " EXERCISE 1 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAtaIs6hW-B_"
      },
      "source": [
        "1. Select a text to analyze and pre-process it using NLTK<br>\n",
        "2. Search for collocations based on POS<br>\n",
        "  a. Examine the most likely collocation types by POS<br>\n",
        "  b. Search for bigram collocations of the type 'Adjective-Noun' like 'powerful lobby'<br>\n",
        "  c. Search for bigram collocations of the type 'Verb-Noun' like 'eat pizza'<br>\n",
        "  d. Search for bigram collocations of the type of your choice<br>\n",
        "  e. Search for bigram collocations of the type of your choice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select a text to analyze and pre-process it using NLTK\n",
        "url = \"https://www.gutenberg.org/cache/epub/67912/pg67912.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "tokens = word_tokenize(raw)\n",
        "print(tokens[:15])\n",
        "tokens = remove_punctuation(tokens)\n",
        "print(tokens[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-udiYJ3EfA9",
        "outputId": "5c4008dd-14de-4677-88c5-1bef69e3f17f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Modern', 'Woman', ':', 'Her', 'Intentions', ',', 'by', 'Florence', 'Farr', 'This']\n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Modern', 'Woman', 'Her', 'Intentions', 'by', 'Florence', 'Farr', 'This', 'eBook', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = nltk.Text(tokens)\n",
        "print(text2[1024:1062])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTN2U2ntKbWV",
        "outputId": "58d417a2-03b1-4111-c782-eea914f12591"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['--', 'The', 'glory', 'and', 'danger', 'of', 'imagination', '--', 'Vicarious', 'imagination', 'in', 'reading', '--', 'The', 'middle-aged', 'suppress', 'imagination', 'in', 'the', 'young', '--', 'Saintly', 'beauty', '--', 'Philosophy', 'Criticism', 'Sensuousness', 'and', 'commonplace', 'life', '--', 'Madness', 'Folly', 'Drink', 'Drugging', '--', 'The', 'imaginative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tagging\n",
        "text_tagged2 = nltk.pos_tag(text2)\n",
        "print(text_tagged2[10:22])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVxaqviLK7xd",
        "outputId": "e4d4282d-cf14-4399-9cf4-17829ccdff96"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Florence', 'NNP'), ('Farr', 'NNP'), ('This', 'DT'), ('eBook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Examine the most likely collocation types by POS\n",
        "finder = BigramCollocationFinder.from_words(t for w, t in\n",
        "    text_tagged2)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Li1pNxEkhD",
        "outputId": "2b1aa421-b51d-4c09-b9d1-0c1482fb504d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NNP', 'NNP'), ('TO', 'VB'), ('IN', 'DT'), ('DT', 'NN'), ('MD', 'VB'), ('NN', 'IN'), ('PRP', 'VBP'), ('JJ', 'NN'), ('DT', 'JJ'), ('DT', 'IN'), ('PRP', 'MD'), ('DT', 'DT'), ('IN', 'IN'), ('JJ', 'NNS'), ('PRP', 'VBZ'), ('NN', 'CC'), ('NN', 'JJ'), ('NNS', 'NN'), ('NNS', 'IN'), ('VBN', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for bigram collocations of the type 'Adjective-Noun' like 'powerful lobby'\n",
        "word_filter = lambda w1, w2: not ((w1[1] == \"JJ\") and (w2[1] == \"NN\"))\n",
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged2)\n",
        "finder.apply_freq_filter(1)\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JblsMQ71EnzE",
        "outputId": "5aaa7688-65a7-405e-82d9-509498f79e99"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('electronic', 'JJ'), ('work', 'NN')), (('same', 'JJ'), ('time', 'NN')), (('public', 'JJ'), ('opinion', 'NN')), (('other', 'JJ'), ('hand', 'NN')), (('economic', 'JJ'), ('independence', 'NN')), (('false', 'JJ'), ('doctrine', 'NN')), (('lifelong', 'JJ'), ('companion', 'NN')), (('free', 'JJ'), ('distribution', 'NN')), (('good', 'JJ'), ('deal', 'NN')), (('emotional', 'JJ'), ('nature', 'NN')), (('exempt', 'JJ'), ('status', 'NN')), (('considerable', 'JJ'), ('effort', 'NN')), (('mutual', 'JJ'), ('contempt', 'NN')), (('fourth', 'JJ'), ('verse', 'NN')), (('lady', 'JJ'), ('doctor', 'NN')), (('matriarchal', 'JJ'), ('village', 'NN')), (('remote', 'JJ'), ('country', 'NN')), (('registered', 'JJ'), ('trademark', 'NN')), (('main', 'JJ'), ('body', 'NN')), (('vegetative', 'JJ'), ('consciousness', 'NN'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for bigram collocations of the type 'Verb-Noun' like 'eat pizza'\n",
        "word_filter = lambda w1, w2: not ((w1[1] == \"VB\") and (w2[1] == \"NN\"))\n",
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged2)\n",
        "finder.apply_freq_filter(1)\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGqxp_53EqtY",
        "outputId": "b4572300-5530-49c9-998d-c566f5dc3579"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('obtain', 'VB'), ('permission', 'NN')), (('incomplete', 'VB'), ('inaccurate', 'NN')), (('stop', 'VB'), ('short', 'NN')), (('cry', 'VB'), ('halt', 'NN')), (('perform', 'VB'), ('distribute', 'NN')), (('_Punch_', 'VB'), ('protest', 'NN')), (('commit', 'VB'), ('adultery', 'NN')), (('copy', 'VB'), ('display', 'NN')), (('s', 'VB'), ('labour', 'NN')), (('practise', 'VB'), ('promiscuity', 'NN')), (('“', 'VB'), ('bear', 'NN')), (('induce', 'VB'), ('courage', 'NN')), (('exercise', 'VB'), ('prudence', 'NN')), (('forget', 'VB'), ('prudence', 'NN')), (('find', 'VB'), ('someone', 'NN')), (('offer', 'VB'), ('nothing', 'NN')), (('seek', 'VB'), ('divorce', 'NN')), (('refuse', 'VB'), ('motherhood', 'NN')), (('mistake', 'VB'), ('interest', 'NN')), (('advocate', 'VB'), ('marriage', 'NN'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for bigram collocations of the type of your choice\n",
        "word_filter = lambda w1, w2: not ((w1[1] == \"NN\") and (w2[1] == \"NNP\"))\n",
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged2)\n",
        "finder.apply_freq_filter(1)\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQzCXiN4EuXP",
        "outputId": "972b2def-5bc7-461e-e332-873cfdda9e15"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('woman', 'NN'), ('’', 'NNP')), (('Project', 'NN'), ('Gutenberg-tm', 'NNP')), (('develop', 'NN'), ('Will', 'NNP')), (('master', 'NN'), ('Curiosity', 'NNP')), (('purpose', 'NN'), ('Are', 'NNP')), (('rapture', 'NN'), ('Hers', 'NNP')), (('reserved_', 'NN'), ('CONTENTS', 'NNP')), (('secretary', 'NN'), ('£350', 'NNP')), (('staff', 'NN'), ('Please', 'NNP')), (('term', 'NN'), ('Very', 'NNP')), (('torpor', 'NN'), ('So', 'NNP')), (('unit', 'NN'), ('Temperance', 'NNP')), (('_Leicester', 'NN'), ('Pioneer._', 'NNP')), (('eye', 'NN'), ('Thousands', 'NNP')), (('livelihood', 'NN'), ('First', 'NNP')), (('redistribution', 'NN'), ('START', 'NNP')), (('service', 'NN'), ('Textile', 'NNP')), (('surrender', 'NN'), ('Again', 'NNP')), (('temperament', 'NN'), ('IV', 'NNP')), (('understood', 'NN'), ('Good', 'NNP'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for bigram collocations of the type of your choice\n",
        "word_filter = lambda w1, w2: not ((w1[1] == \"JJ\") and (w2[1] == \"NNP\"))\n",
        "finder = BigramCollocationFinder.from_words(\n",
        "   text_tagged2)\n",
        "finder.apply_freq_filter(1)\n",
        "finder.apply_ngram_filter(word_filter)\n",
        "print(finder.nbest(bigram_measures.likelihood_ratio, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF6me02aEt_5",
        "outputId": "92977053-db38-47a4-9cac-7f979d3b0e67"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('full', 'JJ'), ('Project', 'NNP')), (('net', 'JJ'), ('Cloth', 'NNP')), (('net', 'JJ'), ('Quarter', 'NNP')), ((\"'AS-IS\", 'JJ'), ('WITH', 'NNP')), (('596-1887', 'JJ'), ('Email', 'NNP')), (('Free', 'JJ'), ('Press._', 'NNP')), (('Theatrical', 'JJ'), ('Literature', 'NNP')), (('VOTE', 'JJ'), ('Latent', 'NNP')), (('_Eugenic', 'JJ'), ('Review_', 'NNP')), (('_Liverpool', 'JJ'), ('Mercury._', 'NNP')), (('_T', 'JJ'), ('P.', 'NNP')), (('peer', 'JJ'), ('Girls', 'NNP')), (('shame', 'JJ'), ('White', 'NNP')), (('-+', 'JJ'), ('|Transcriber', 'NNP')), (('French', 'JJ'), ('_dot_', 'NNP')), (('_Les', 'JJ'), ('Avariés_', 'NNP')), (('childless', 'JJ'), ('Ninon', 'NNP')), (('exact', 'JJ'), ('IX', 'NNP')), (('frank', 'JJ'), ('Unfortunately', 'NNP')), (('life-consciousness', 'JJ'), ('Henri', 'NNP'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcf6lTMSW-CA"
      },
      "source": [
        "# GET THE FREQUENCY OF EACH POS IN A TEXT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYdMq05W-CA"
      },
      "source": [
        "The following code was taken for our NLTK book, Chapter 5, Section 2.7<br>\n",
        "'Unsimplified Tags': https://www.nltk.org/book/ch05.html<br>\n",
        "Consult that section and that chapter to learn more about working with tagged corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_5WKJNlW-CA"
      },
      "source": [
        "Find the most frequent nouns of each noun POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "2C_95u_IW-CA"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "vq1Uk1pIW-CA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e8078e-a801-4830-cabf-6ad71d2ef6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN [('“', 530), ('”', 464), ('s', 355), ('time', 343), ('thou', 264), ('master', 242), ('curate', 232), ('worship', 227), ('way', 226), ('man', 211)]\n",
            "NNP [('”', 1278), ('“', 1185), ('Don', 1123), ('Quixote', 895), ('Sancho', 687), ('’', 682), ('THE', 217), ('God', 195), ('Fernando', 133), ('Camilla', 133)]\n",
            "NNPS [('Christians', 28), ('States', 15), ('Moors', 9), ('Cervantes', 9), ('Maritornes', 8), ('Yanguesans', 4), ('Algiers', 3), ('Indies', 2), ('”', 2), ('Arabs', 2)]\n",
            "NNS [('words', 170), ('things', 128), ('eyes', 127), ('books', 123), ('days', 105), ('arms', 99), ('hands', 93), ('others', 84), ('men', 80), ('people', 73)]\n"
          ]
        }
      ],
      "source": [
        "tagdict = findtags('NN', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDT6f1k8W-CA"
      },
      "source": [
        "Find the most frequent verbs of each verb POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "EvFEIk3iW-CB"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "He_EV47SW-CB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccd2416-1809-4bfc-c82a-7e2c2447ea47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VB [('be', 1630), ('have', 552), ('see', 282), ('do', 270), ('make', 240), ('give', 203), ('go', 197), ('take', 197), ('say', 185), ('tell', 177)]\n",
            "VBD [('was', 2099), ('had', 1389), ('said', 1103), ('were', 726), ('did', 345), ('came', 220), ('made', 216), ('s', 206), ('replied', 170), ('saw', 162)]\n",
            "VBG [('being', 256), ('having', 160), ('going', 131), ('seeing', 129), ('saying', 113), ('taking', 83), ('coming', 67), ('making', 61), ('giving', 59), ('calling', 57)]\n",
            "VBN [('been', 521), ('made', 157), ('done', 146), ('seen', 136), ('taken', 102), ('given', 97), ('left', 81), ('found', 80), ('come', 76), ('called', 76)]\n",
            "VBP [('have', 726), ('are', 546), ('am', 292), ('say', 216), ('know', 184), ('do', 179), ('”', 94), ('see', 72), ('’', 59), ('“', 55)]\n",
            "VBZ [('is', 1870), ('has', 466), ('does', 81), ('seems', 73), ('“', 68), ('s', 63), ('comes', 51), ('says', 48), ('makes', 42), ('”', 34)]\n"
          ]
        }
      ],
      "source": [
        "tagdict = findtags('VB', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELTXWBfW-CB"
      },
      "source": [
        "Find the most frequent adjectives of each adjective POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "-6egXJpnW-CB"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9ucSmh3IW-CB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebe6687-bfab-476c-b9d8-a0e0d3307d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JJ [('such', 373), ('good', 361), ('other', 348), ('great', 287), ('thy', 222), ('same', 217), ('own', 207), ('thou', 192), ('little', 191), ('many', 184)]\n",
            "JJR [('more', 331), ('better', 84), ('greater', 51), ('less', 46), ('worse', 33), ('easier', 8), ('higher', 8), ('longer', 7), ('stronger', 6), ('happier', 6)]\n",
            "JJS [('best', 71), ('least', 69), ('most', 55), ('greatest', 29), ('lest', 28), ('mayest', 19), ('highest', 15), ('manifest', 14), ('knowest', 13), ('strangest', 6)]\n"
          ]
        }
      ],
      "source": [
        "tagdict = findtags('JJ', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkla2WJIW-CB"
      },
      "source": [
        "Find the most frequent adverbs of each adverb POS type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "l9knfLKlW-CC"
      },
      "outputs": [],
      "source": [
        "def findtags(tag_prefix, tagged_text):\n",
        "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
        "                                   if tag.startswith(tag_prefix))\n",
        "    return dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "2prhliF_W-CC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93204658-59f2-47fe-fc51-93c9a97646c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RB [('not', 1992), ('so', 1153), ('now', 351), ('very', 305), ('as', 305), ('then', 305), ('well', 262), ('even', 216), ('only', 206), ('once', 179)]\n",
            "RBR [('more', 273), ('less', 29), ('better', 19), ('longer', 8), ('further', 5), ('closer', 3), ('laughter', 2), ('faster', 2), ('worse', 2), ('thither', 1)]\n",
            "RBS [('most', 120), ('best', 8), ('hast', 2), ('mayest', 1)]\n"
          ]
        }
      ],
      "source": [
        "tagdict = findtags('RB', text_tagged)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4PTuHbW-CC"
      },
      "source": [
        " EXERCISE 2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaHclYdjW-CC"
      },
      "source": [
        "1. Select a text to analyze and pre-process it using NLTK. It can be the same text<br>\n",
        "      that you used for EXERCISE 1<br>\n",
        "2. Find the 10 most frequent types of:<br>\n",
        "  a. nouns<br>\n",
        "  b. verbs<br>\n",
        "  c. adjectives<br>\n",
        "  d. adverbs<br>\n",
        "  e. a POS category of your choice<br>\n",
        "  f. a POS category of your choice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nouns\n",
        "tagdict = findtags('NN', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBmLdaP8N6aY",
        "outputId": "03cef543-fea1-4af5-e447-f308c9b9503d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN [('woman', 57), ('work', 54), ('life', 51), ('love', 39), ('man', 33), ('time', 29), ('wife', 27), ('marriage', 26), ('s', 24), ('world', 23)]\n",
            "NNP [('Project', 79), ('Gutenberg-tm', 55), ('’', 50), ('Gutenberg', 28), ('”', 25), ('Foundation', 23), ('THE', 21), ('“', 20), ('United', 16), ('OF', 13)]\n",
            "NNPS [('States', 16), ('Jews', 4), ('Mahommedans', 2), ('Courtesans', 1), ('Arabs', 1), ('Brothers', 1), ('Races', 1)]\n",
            "NNS [('women', 120), ('men', 56), ('people', 30), ('children', 29), ('works', 28), ('terms', 20), ('laws', 19), ('facts', 15), ('years', 13), ('donations', 13)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verbs\n",
        "tagdict = findtags('VB', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egjGPjYeN6Iw",
        "outputId": "74d83490-95d1-4078-ecb6-025de9405479"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VB [('be', 152), ('make', 32), ('have', 23), ('do', 16), ('get', 16), ('think', 14), ('come', 10), ('take', 10), ('see', 10), ('find', 9)]\n",
            "VBD [('was', 32), ('had', 18), ('were', 17), ('said', 16), ('s', 14), ('made', 4), ('brought', 4), ('called', 4), ('found', 3), ('gilt', 3)]\n",
            "VBG [('making', 15), ('working', 9), ('including', 8), ('using', 7), ('being', 7), ('having', 7), ('becoming', 5), ('writing', 5), ('saying', 5), ('distributing', 5)]\n",
            "VBN [('been', 30), ('made', 16), ('found', 10), ('used', 8), ('written', 8), ('located', 7), ('given', 7), ('brought', 7), ('done', 7), ('set', 7)]\n",
            "VBP [('are', 183), ('have', 90), ('do', 33), ('want', 16), ('am', 14), ('know', 12), ('think', 11), ('say', 9), ('come', 7), ('make', 6)]\n",
            "VBZ [('is', 318), ('has', 69), ('makes', 12), ('does', 10), ('seems', 9), ('gives', 8), ('comes', 7), ('appears', 6), ('means', 5), ('becomes', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adjectives\n",
        "tagdict = findtags('JJ', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TGiLPWN50h",
        "outputId": "87f84508-f804-45fc-c79e-b13c79eb9a66"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JJ [('other', 61), ('electronic', 27), ('great', 23), ('public', 22), ('present', 20), ('many', 20), ('first', 19), ('good', 19), ('old', 19), ('same', 19)]\n",
            "JJR [('more', 17), ('better', 8), ('less', 6), ('greater', 2), ('poorer', 2), ('love', 1), ('worse', 1), ('harder', 1), ('lower', 1), ('weaker', 1)]\n",
            "JJS [('most', 11), ('greatest', 6), ('least', 5), ('Most', 3), ('worst', 2), ('deepest', 2), ('best', 2), ('widest', 2), ('strongest', 1), ('earliest', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adverbs\n",
        "tagdict = findtags('RB', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6YAjd87N5kg",
        "outputId": "077153eb-49de-4ce0-e1d6-368d5e66be2a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RB [('not', 128), ('very', 40), ('so', 31), ('only', 27), ('often', 18), ('now', 13), ('too', 12), ('as', 12), ('really', 12), ('never', 10)]\n",
            "RBR [('more', 24), ('better', 5), ('less', 5), ('longer', 2), ('witness', 1), ('harder', 1), ('harmless', 1)]\n",
            "RBS [('most', 20), ('_not_', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Determiners\n",
        "tagdict = findtags('DT', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApzRBphN5No",
        "outputId": "5edc26af-4f4d-4405-f40b-1ba546cc50e4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT [('the', 1178), ('a', 389), ('The', 148), ('this', 73), ('all', 65), ('any', 60), ('an', 46), ('no', 41), ('these', 34), ('some', 26)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO\n",
        "tagdict = findtags('TO', text_tagged2)\n",
        "for tag in sorted(tagdict):\n",
        "    print(tag, tagdict[tag])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MfWb8M8N4k3",
        "outputId": "7c0d8853-42e6-4da6-e659-4d92c1675e80"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TO [('to', 515), ('To', 13)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWsxPX5CW-CC"
      },
      "source": [
        " END LAB"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LING_490_Lab_7.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}